{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjwq6pS-kFz"
      },
      "source": [
        "# Stock NeurIPS2018 Part 2. Train\n",
        "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*.\n",
        "\n",
        "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
        "\n",
        "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zXutMgqOS"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D0vEcPxSJ8hI",
        "outputId": "479a18ff-777a-4fe8-fea1-eff2daa52b33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/site-packages (4.3.0)\n",
            "Requirement already satisfied: wrds in /usr/local/lib/python3.11/site-packages (3.3.0)\n",
            "Requirement already satisfied: packaging<=24.2 in /usr/local/lib/python3.11/site-packages (from wrds) (24.2)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /usr/local/lib/python3.11/site-packages (from wrds) (2.2.3)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /usr/local/lib/python3.11/site-packages (from wrds) (2.9.10)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /usr/local/lib/python3.11/site-packages (from wrds) (2.0.38)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/site-packages (from pandas<2.3,>=2.2->wrds) (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas<2.3,>=2.2->wrds) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas<2.3,>=2.2->wrds) (2025.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/site-packages (from sqlalchemy<2.1,>=2->wrds) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.17.0)\n",
            "Requirement already satisfied: pyportfolioopt in /usr/local/lib/python3.11/site-packages (1.5.6)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt) (1.6.2)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt) (2.0.14)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt) (2.2.3)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt) (2.2.3)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt) (5.24.1)\n",
            "Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt) (1.15.2)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt) (0.6.7.post3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt) (3.2.7.post2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas>=0.19->pyportfolioopt) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=0.19->pyportfolioopt) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas>=0.19->pyportfolioopt) (2025.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt) (24.2)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/site-packages (from osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt) (0.1.7.post5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pyportfolioopt) (1.17.0)\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-3d1_9hxf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-3d1_9hxf\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 8cf3cacc6f570d26b430e403ea522c8fe9e6876a\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.6)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-5xfbjl5o/elegantrl_a554d2a0d2f74c278c7958c26a1cd5c7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-5xfbjl5o/elegantrl_a554d2a0d2f74c278c7958c26a1cd5c7\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 3bdc958c8e624b61a9e661832b01fef816924f61\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: alpaca-py<0.38,>=0.37 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (0.37.0)\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (3.2.0)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (3.1.60)\n",
            "Requirement already satisfied: exchange-calendars<5,>=4 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (4.9)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (1.9.7)\n",
            "Requirement already satisfied: pyfolio<0.10,>=0.9 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (0.9.2)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (1.5.6)\n",
            "Requirement already satisfied: ray<3,>=2 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.43.0)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (1.6.1)\n",
            "Requirement already satisfied: selenium<5,>=4 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (4.29.0)\n",
            "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.6.0a2)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (0.5.4)\n",
            "Requirement already satisfied: webdriver-manager<5,>=4 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (4.0.2)\n",
            "Requirement already satisfied: wrds<4,>=3 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (3.3.0)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.11/site-packages (from finrl==0.3.6) (0.2.54)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (1.0.3)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (2.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (2.10.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (2.32.3)\n",
            "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (1.8.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.6) (10.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.2.3)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.8.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.11.13)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.11/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (24.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.6) (65.6.3)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.6) (2024.12.14)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.6) (44.0.2)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /usr/local/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.6) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.11/site-packages (from ccxt<4,>=3->finrl==0.3.6) (1.18.3)\n",
            "Requirement already satisfied: pyluach in /usr/local/lib/python3.11/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.2.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (1.0.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2025.1)\n",
            "Requirement already satisfied: korean_lunar_calendar in /usr/local/lib/python3.11/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.17.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (2.0.38)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /usr/local/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /usr/local/lib/python3.11/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (0.5.2)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (9.0.2)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.10.1)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2025.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.15.2)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.13.2)\n",
            "Requirement already satisfied: empyrical>=0.5.0 in /usr/local/lib/python3.11/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.5.5)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.6.2)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (2.0.14)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /usr/local/lib/python3.11/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (5.29.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (1.5.0)\n",
            "Requirement already satisfied: aiohttp_cors in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.5.6)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.70.0)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.11.4)\n",
            "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.21.1)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (7.1.0)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (20.29.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (19.0.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2025.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.5.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/site-packages (from selenium<5,>=4->finrl==0.3.6) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/site-packages (from selenium<5,>=4->finrl==0.3.6) (0.12.2)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/site-packages (from selenium<5,>=4->finrl==0.3.6) (4.12.2)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.1.1)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.19.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (7.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.10.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.1.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/site-packages (from webdriver-manager<5,>=4->finrl==0.3.6) (1.0.1)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /usr/local/lib/python3.11/site-packages (from wrds<4,>=3->finrl==0.3.6) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.3.6)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.13.3)\n",
            "Requirement already satisfied: th in /usr/local/lib/python3.11/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.6) (0.4.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.11/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6) (4.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (2.5.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (0.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.7.post3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.7.post2)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.11/site-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.0.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.50)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.19.1)\n",
            "Requirement already satisfied: stack_data in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /usr/local/lib/python3.11/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.14.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.9.0.post0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.6) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.6) (2.27.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.6) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.6) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/site-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6) (3.1.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.11/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.6) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.6) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.6) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.6) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (0.23.1)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.11/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.24.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/site-packages (from smart_open->ray[default,tune]<3,>=2->finrl==0.3.6) (1.17.2)\n",
            "Requirement already satisfied: niltype<2.0,>=0.3 in /usr/local/lib/python3.11/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.6) (1.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.69.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/site-packages (from osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.1.7.post5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/site-packages (from pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.6) (0.14.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/site-packages (from stack_data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/site-packages (from stack_data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/site-packages (from stack_data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "## install required packages\n",
        "!pip install swig\n",
        "!pip install wrds\n",
        "!pip install pyportfolioopt\n",
        "## install finrl library\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrSrQv3i0Ng"
      },
      "source": [
        "# Part 2. Build A Market Environment in OpenAI Gym-style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHhM2U-XBMZ"
      },
      "source": [
        "![rl_diagram_transparent_bg.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjoAAADICAYAAADhjUv7AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAB3RJTUUH4gkMBTseEOjdUAAAHzdJREFUeNrt3X+sXWW95/H31zSZ/tFkesdOpnM9wU5bM72ZGkosCnKq4K20zJRRIsZThVgyIhZhIlEKXjE4USNFHXJD6EHQ2IlIa6gBB2Y4hSo/eu4VpV5q7A1MPK3Vqdqb4Tqd3P7BH02+88d6dlld7NOe32f/eL+SnXPO/rHO2s9a+3k++3metVZkJpIkSb3oTRaBJEky6EiSJBl0JEmSDDqSJEkGHUmSJIOOJElSzQKLQOocEbEYuAY4H1gLrPZz2pFOAYeAA8Avgd2Z+arFInVgvep5dKSOCTmbgGFgFPgb4AXgYGaesnQ6blstKCF0LXAJsBG4OTP3WDqSQUfSGxvOrwObgOszc9QS6brtd1EJqQcy83pLROocztGR5r+R3FRCzoWGnO6UmS8AFwJrI2LIEpE6qI61R0ea15CzGPgVsNmQ0xPbcw3wNHBBZh6zRKT5Z4+ONL+uAUYNOb0hMw8CewB7dSSDjiTgXcBei6Gn/LhsV0kGHanvraU6ukq94yCwxmKQOoNzdKT5/ABGZGaGJeF2lTQ77NGRJEkGHUmSJIOOJEmSQUeSJMmgI0mSZNCRJEky6EiSJIOOJEmSQUdSX4iIwYjIiPBMo5IMOpJ6zkcp1+aKiHm7cGUtcA26SSSdzQKLQNIkbAXWld+3ALstEkmdzB4daQ5ExOIeeA9DwOHMHAV2AhsiYnmb52Wb21jt8cHxHiuP74iIkYgYajxveetxYH95+v7y2A73MkkGHWn+fCYiXoyIqyOiW3tStwBPld9/Xn5e3QgpY8BwZka5qOVeYG9mrqyFpf3AitpzxpphB9gAbGks5ymAzLyR13uV1pXn3OguJsmgI82f48Ba4BHg5Yi4KSIWdsvKl96UDcDDJWwcKeHjk43nrGg9p9hZXtfyFeC28vr6fSsa820OZ+bGxnJWtOtBkqSzcY6ONP0QsAAYAJbVfr4FWAgsARYBS2svWQncC3y2i97m1SXgjDbCx66IGMzM0cw8EhGHqSYst563pQSilhXA9ojYPsn/f6z8/HPgSJfsF78pv75Wgm7L0UYA/n257xhwLDNf9VMlGXSkuW60FgKrgTXAv62Fmtat1VC1fv49cBI4UW4bgNvL4k6VkPBF4I9dUgSfLOXQ7rDyerAB2BoRW8vvh1vDVjW3Zebdvb7PZOa/iYiBWj3bCr2Un0tKGH4r8K5WSI6IJa3QU/an3wOHgEOZ+YqfRsmgI0031CwqgWYtcH75fTXwCnCwhJhf1L6BH51gULodGAFuzcxD5f5uKI9Bqp6YdY0endbE4K3Aja3nlTk14zlcQuJ0/aFLws6x2p9HJ1HmA40w/QHgCxGxrISeA2U/PFAC0Ck/uZJBRxqvUVkGbATeW8LNQGlMDpZAcx/wSmaenMa/OQq8PzP3dWERfZTXj7Zqep6qB2ewFT7a9Prsrc23eYBq6Or5zNxdnr8ceKpNz89EvJsze5N6QglIx8YJzKtrIfwGYGVEHAVeAJ4Dns3M436ypfK5yfQEp+q7YLMUuJRqOOlSquGDZ4Efz/U35IjIc/SAdEJ5JWcZbiqPD2fmjeX3M3p+yhFVT7WOjCpHXu1qNOxRe/4O4PJ68ClBan992RGxDWjN9emo4bC53q4RsRoYLGH9UuBVYF8t+Jzwky+DjtS7wWYhsL4Em/VUPTatYPNsZh7slwZxlt/LGwJK7f7ljaOoen2fm9ftGhFrSuD5yxKAXmns8w51yaAj9UC42Qh8CNhENQz1HNUcmQOdUtH3WNBp9bCsaB0+XoalDtMnE5A7cbuWowLXls/DXwKrgMeAHxh6ZNCRuqtxWVAq84+UcHOgVOaPdeohu70UdMr7aU1Ortvcmo9j0OmIdRugOl3Ah0ro2Q38YJw5WJJBR+qAint9CTcfpOqi/yHwUDecj6TXgo66a7uW0LOlhJ4lwJ4Sel5wK8qgI81vBb0Y+ATVUScngO8DexqH89ogyu068XVeCQwBH6c6B9R95QvDa25RGXSkuauMVwOfLhXyE8B93fzt06Bj0OnQ9d9UvkQMAt8un7Ojbll1I691pW6odBeUi2HuBx4Hfgu8LTOvtYtdmnmZ+URmXglcSHW+tZci4vESgKTuakPs0VEHB5zFwE3lm+Uhqq70kV46SsQenZ7dd3ttkvlCqrk8n6Y679R9wP0Oa6kb2KOjjgw4EfEl4NdUlx64LDOvKN8yPRRWmmOZ+Vpm3p+Zb6c6qu4DwG8i4jMlBEkGHWmSAeetwMWZeV1mjlk6UseEnn2ZeRmw2cAjg45kwJF6NfA8a+CRQUc6e8BZGBF39HnAGSuH9ap39uuVtLkgZ58FnpvKCTwlg476tjHYCPwKeAf93YNzgOoQXvWONWW79pU2geelcjFWyaCjvgo4yyLiUeBe4ObMvKrPh6h+RnXFafWOS4Bf9OubL4Hn/cBXgV0R8b2IWOpuIYOOej3gtIapXiqNwNszc8SSYTewMSIusih6Yj9fBVwDPNTvZVGub/Y24ChV785nHM6SQUe9WvnXh6kuyMyveP6N043BceBmYNhGoOv38wXAd4HPexbh0/v3a5n5RWAdsAGHszQfn01PGKhZrPgXUw1RXUQ1TGUPzvhl9SCwFrguMw9aIl23/VaVkDOWmddaIuOW0weBe4B9wC2ZedJS0WyzR0ezVaFdSjVMdQKHqSbyzfd6YDvwdETcWy554dFYnb2Pryzb6R5gP/AdQ8459/PHgLcDp6h6dxyy1ex/Vu3R0QxX/guArwFXA9dn5j5LZVLlN0B1qv13UB2NtcRS6VgngFGqOWc7Ha6a9L6+CXiQ6nISd3nWcxl01A0V12rge8BYCTknLBVJZ6kzlpawswTYbFjUbHDoSjNVYX0GeBr4ZmZ+2JAj6Vwy83i5Svr3gRcjYoulohlvn+zR0TQDziKqXpzFwLWZecxSkTSFumQVsAs4SNUj7FCWZoQ9OppOxTRANQnzOPB+Q46kqcrMV6gOQ18CPONJBmXQ0XyHnIuAnwLfysytfvuSNANh5yRwFdUk75+WeX/S9Norh640hZBzDfB1qqEqj6qSNBv1zBaqc+5cm5lPWCKaKs/EqslWPl+mOnT8stLVLEkzLjN3RsQY1fWyVmfmXZaKptRu2aOjCQacBVQTBRcDHlUlaa7qnmXA48C+zLzFEpFBR7MZcpYCV3jadklzXActAp4EDhh2NFlORpYhR1JHK/XOFcDacskNyaAjQ44kw45k0JEhR5JhRwYd9R1DjiTDjgw66j0R8SVgwJAjqcPDzqURcbslorPxPDpqhpwPAjcAFxhyJHVy2ImIq4D9EXEwM0csFbVt1zy8XLWQs5rq2lVXZOYLloikLqi3LgUeBS72JKZqx6ErtSqLxaWyuNWQI6lbZOazwK3Ao6Uek85s3+zRUTnC6nHgaGZutUQkdWE99iDV3MIrvciw6uzREcCXgYXAzRaFpC61tdRjX7ModEYItken778FXQQ8AlyYmcctEUldXJ8tBV4ENmfmqCUisEen3yuFBcCDwC2GHEndrtRjNwMPRsRCS0QGHd1ONS9nj0UhqUfCzmPAoVK/SQ5d9e2Gj1hFdSj5BZl5zBKR1EP12wDwErDOQ85lj07/ehD4L4YcSb2m1GtfBL5bhuhl0FGffdv5FNVZse+3NCT1aNi5HzgFfMLSMOioO8PKWETsmMLrFgBfoDoxoOeakNTLbgbu7JaJyRExGBEZEUNuug4JOhGxo2yU7IaNExHLG+vbum3ro20+RDUB2UMvJfW0zDwIvFLqvdloU1ptyKCl3YNBp/QmbM3MaN2Ar0TE8kaoGJrkcpfPQWjaXFvnzcD2Pgo7nwW+6a4vqU98s9R7Mx1yhoDD5fbRKbx+JCJGGsFstLRNu91sHRB0gMuB4cZGWpmZR7os8e8uO+r7en1jR8R6YFE5/FKSel5mPlHqv40zvOgtwAPl5qVzejTotMJOuwZ1WwkPALtKD81I7fGxxtDR4ARfN9R43cgshoKR8Ybl2g13lfc00rhvR0SMTXCZrZ6sweaQWuO+6fR2fRbY7m4vqc/MaK9OGbnYAOwpN9rVy23arG2tur68fkPtseXjjWi0aTt2tGlrRtr8v+Vu+irtTulGNeaZ5TbU5vHl7R4DxoDB2t/bqtU45+vOeF5tWSOTWOc3LLu13MZ9Y8CO2t+D9ecAO4CxxnJHxlm/beX3kcb/aJXf8sa6ZaN8Wvdvayw36+s4gfe+BvgjsHCq29ybN2/euvFGdQ2sPwJrZmh52xptwBvaolodP9h43vJamzAygTZqrP6/yn1Zf21pk5r3jTRf16+3N00jIO0uc1zqvS9DE3jdysZE2L+tJeSz2U41n6bujpKIJ5taW+ubwPb6mGh5Dysy88baOo8Ce0tXJcDzwIra/30n8BNgb613ahBY0Xp/mbmxMe768/LzzxvrdlujfK4ur7+7Xoa1nq+J+gjwrcx8zXgvqc++0L8G3Ad8bIYW+UmqIauWB9q0RV8Bhuv1+WSnd7TaozajJ5vb/L/DmVkfntvZaKccuprGDtSa1Hu4BIihCWy8rAWN/eM0+M1uwjMCSnntrimu9uayzita3X61x85rrmOtm/F0yKsFHID3lEDzE+Dd5b53lx1vtDG81VpeK6gMNNbtd42/31dC1nStLwlfkvrRCDDteTrNL7HFnvoX02IFcHSa/+680uY0w9Gxc7WbE3yOQWeSgafVy7DlbIGlNPLDtYC0brIBpc3tyBTX+QhwG7C1mXrH+T/1D8lw7b1uLYHmb0vSbwWUB+rhjqobMeohay6UK/quAg5Y10nq016dA8DSUh9OR+sIq/1tvrh+0pLu4aBTc2ScBFpPlt84R/gY777zZmHnbw0Jfa78/F0rlJ3jpc9TdR0OtnbyEnZW1CaqNYflvjLF8lzZ5v7JBKX1wD5PECipz+1j+r06W6mmGETj9CqbS/3fOqfOYWDZudrKcxivPWqNBPzBTTpLQad1FFDjvm2l8X248fT31H5vbZR6997+cf7Nexp/D1Od72awsR71o7K2TXGm+XDZeevDUk813t+O+rBc7Xl3NJ67l2pi2Olhq1pQq59r4akJrtvD5cOzrbYuY5N8fxuYmeEvSepme4H3TvXFtTZgT5uHW/MuW9MXHqAaLai3WWON9mnDOb6It9qZHbVlLKeatjHcbadz6aqgUxrwzY05LNupJvHWJ9JuLhs6I2JH2SitE/S1Xre5zb8443Xlf95INcxU7y7c2RhOmqqH6ztxa5J14/0dbXMSp71lR32+dt9Pyn0PNJ67rvaesgSkCZd1o8y2TDK4rC/fZCTJHp2p2wLsPcvIw17K8FUZLWi2WQ+0Xts64KX22HhtQAArG8Nkt9UPmNE5Amo5DK033kzp3Zmh8NMrZbIS2J+Z/9rSkGSdGL8CrszMo5ZGf1jQQzvvIFVPygo36xkGqM7DIEmqjkZaxvSPiFKX6KWrl3+UqjvPMcs3Bp3jFoMkQakPl1kM/aNnenQcrxzXEoOOJJ32W2CpxdA/3mQR9Lx/BfyDxSBJUL74vcViMOiodyzl9TNkSpJB541npJdBR11smUFHkk47ASy2GAw66h2vUs3TkSTJoKOecxwn3klSywAeWm7QUU/5B6oJyZIkj0Q16KjnHMMeHUlqeTMeiWrQUc8FnWUWgyQB1dDVqxaDQaerRMSby9XFWxfh/FPrYqBTXN5gucrsn8rvyyNid1n27i4rHufoSNLrOuaUGxFxQ2lrMiJejIihLmxjOl6vnBn5PqprXC2p/X3hFHe85VSXk3hXSf03UR2K+LHy85kuK5sxYCAiFmfmCXd5SX1uLfBKB4ScrwJ/BWzOzN0RMQTsAobdRDNc1r1w9fKI+BNwV2beXf4eBG7KzKFpLjeBw8C7MvMfu7h8ngT+W2b6TUFS/zZ4ERcB92bmhfO8HoPAfuBTmfmtRpuz2bp6ZvXKHJ2ngNsj4nyAzBydgZBzfvn1jm4OOcX/oLqyuyT1s/XAvg5Yj48C/7cRcgbLry+7mQw67fwVVc/LMxFxQ5vQcsMU5uxcVH4+PUPLm08j5QMuSf1sA7C3A9ZjqHxBr/t3Jfz8stHetIa11M9BJzOPABuBu4D7y9hn3VVM/gRR5wMHxunNmcry5rN8xoDXImK1u7ykfhQRi4HVwGgHrM6fAX/XuO9W4OeNdX4zcDlexqe/g05EvFga838sc3SGgXc0Ht8AbC8z27dNcNGXAy+O8/+msrz5NgJscpeX1KfWA6OZeaoD27EbgH8B/KR23/nAz0oo2l/am0E3Y58FnbIjrG1165Ujpi6kumhby0fKzyWZGa0Jy+X5bYNKSdErgF+2+bfjLq/D/QD4uLu8pD71MeBHHbIuh4H3lfZmCDiv1v4MRsRXyxDWHVQjC1Fuo27GPgs6wD+VBnxHma1+gKoX5tO157yT8YegxvMX5efft3lsKsubd+UD8lpEfNDdXlI/iYiVwCDwUIes0n8G3lmOGD4vM79ANWdnO3AF8F/L897DG+fyaLLbvxcOLz/HDr4b+Ltmz0vpAvzvwNoyx2day+uSsrgG+E+ZeZm7vqQ+CjrDwKuZ+cUuW+8/Af/Rnpzp6YdLQKwFflfOblw/IusO4LLJhJxzLK8b7AZWRsRad31JfRJyllAd5XRfl633cqr5OX8ow1n/3q1p0BnPD6jONrkD2NO6MzM3Ng/jm87yukGZhPfXwG3u+pL6xE3AY5nZVVcsL1/CD1DN57kiM/+nm3KKobHXh670hm8Ji4FfAxeXw84lqVfru4XAb0pQOGiJ9CevXt5nyvWudmKvjqTe9yngkCGnzwOvPTp9+S1nEdVpxq/NzGctEUk9WM8NAC8B6zLzFUukf9mj04cy8ySwFRguXbuS1GvuAe4z5Mig079h5wngEPAFS0NSL4mITVSXe7jL0pBDV/1dGSwFfkV1mP0hS0RSD9Rri0q9dp1D8wJ7dPpaOdzyVuDBiFhgiUjqAXcCzxpyZNBRK+zsBE4Ct1sakrpZGbIaKl/gJAD8Fi+Aa4GfRsShzHzM4pDUhSFnFfA94KrMfNUS0el9wzk6KpXEauBpPLGWpO6rvxZRXdD5m5n5bUtEBh2NV1lsAu6lOmvycUtEUhfUWwuAR4HjmXm9JaImh650WmY+ERFrgEci4rJybSxJ6mR3AkuAqywKtQ3D9uiozTek7wGnMvM6S0NSB9dVV1OdGPBCe6Fl0NFkKo+FVPN1DmTmLZaIpA6spwaBR4ArM/OAJaLxeHi53iAzXwOuANZGxD2WiKQODTkfNuTonPuLPTo6S2WyCHgSe3YkdU69tKbUSx/OzFFLROdij47GVS7+ac+OpE4JOauAx6ku72DIkUFHhh1JPRVyngZuycwRS0QGHc1W2Bn2uliS5jjkDNZCzh5LRAYdzWbYWQo8GRFLLBVJcxBytlANV2015Migo1kPO5l5FfA3VNfGWmWpSJrFkPNlqhMCrsvMJywRTWk/8qgrTbECGqI6Udf1VkCSZrh+WUR1gc6lVBfp9GSAmjJ7dDQlmbmbaijr3oi43RKRNEMhZymwHzgJXGbIkUFH8xl2DgIXAx+IiEectyNpmiFnDfAS8KPMvLacvFQy6Ghew85xYB1wHHgpIjZaKpKmEHI+R3UiwJsz80uWiGZs33KOjmawotoIPAg8BtzqtzFJE6g3Bqjm4wBcm5nHLBUZdNTJldaSEnZWAZvL8JYmXn6LgWuA84G1wGrA8xZ1nlPAIeAA8Etgd2a+arFMen8fAu4FtmfmNywRGXTUTRXYJ4CvA9uBb2TmKUvlnGW2CRgGRqkO4X8BOGjZdeS2WlBC6FrgEmAj1ZCL53mZeKC/F1hD1YvjFyIZdNSVldkyYFf59ntdZo5ZKuOW1deBTVSH63sNn+7bfheVkHogM6+3RM5aVut5fYj78w5xa7Y5GVmzJjOPUk1U/hHVCQa/Vs6PoTMr/k0l5FxoyOnaff0F4EKqy6QMWSJt9/OBiNhVAuF1mXmLIUcGHfVCA3CqjL2/HRgAXrYhOKPyX1wq/uvLZTbUxfs6cB3VuaUGLJHT+/iCiPgM1WHj/wt4e2Y+a8lozvZBh640x5XeINXY/Emqa9cc6vPyuAm4JDM3u3f0zDYdBg47ufb0530YOEY1h8nha805e3Q01996R6m6+L8PPFOuhr64j4vkXcBe94ye8uOyXfs54CyJiAep5uh9NTOvMOTIoKN+CjunMvN+4G3lrl9HxJf6NPCspTq6Sr3jINXRRP0YcBZHxJeAl6l6bf+iXC5GMuioLwPPiczcSnUZibf2aeBZlZmvuDf01H49Bqzs04Dz6/JZvrhMNnbemQw6UmaOZeZ1tcDzch/38EjdHnA8lYQMOtI5As8FwD838EgGHMmgo14MPMcz85Za4Pl1RNwbEastHWleA86qiLjHgCODjjSzgedtwG+BRyPimYi4upyCX9Lsh5sFEbEpIp4Efkp1pnMDjrpnH/Y8OuqySncj8Gmqo1q+A9yfmce7+P1kZoZbtuf2067frmXI+BPl83YCuA94yLMZq9vYo6OukpkjmXkl1aUl/hnwUkQ8Ur5xLrSEpGkHnDXlHDi/Ad4BbM7MCzLz24YcdeU+bY+OurxSXghcDXyc6pw0TwA/BEa6oVK2R6dn98uu2q5l/ttHgNblWb4D7Ozm3lLJoKNebFyWANcAH6Ia2nqs00OPQcegM4/ruLIEmw8BS4CdwA8z86BbUAYdqfMbmgGqnp6PAKtL6PkB8GwnncTMoGPQmeP1WgVsLJ+LAWBPCTejbjUZdKTubXRa31z/A1VPz0Gq60s9C7wwn709XfLNfzlwGLgtM+92j+qe7RoRS4FLgQ3lJ8C+Wug/5daSQUfqrQZoETBYq/hXAqPAc8C+zDzQTQ1iROwAtrZ5aDgzbzTo9FfQabN/D5Rg8xzVEO5Rt44MOlJ/NUiLqbry31sahqVUPT4HgV8Ah4BDs/XNd4aCzuWZudKtOePbZgx4aiqB8WzbNSIWZ+aJGVi/hcAqqkn45wMXleD+AtUV1Pc530YCT7qmvlYanN3l1urqX0s1r+cDwJ3AQEQcKuHnl+XnoZlorNRXwelS4B7gr6km/k7mtYuohl3XlFCzFlgGvAIcKPvld2YzlEvdyvPoSGcGn+OZ+URm3pWZH87MtwH/ErilNCbnA/cC/zsi/ikiXo6IJyPiwXJdri0RcWlErOyE8/pExPKIyIgYjIix8nuWnqD640ON1w22XtfqoYiIbfUei4gYqi1zR70npPZ/znhdeXwkInZExLb68xrPGSr3L28sq74+2W7d2zyeZfjtXMseqr93YAWwtd36TXIbrI6Ix4FnSlBp95yBiLiorNvnyiVPHo2IlyLi/1BdcuFOqssuPAdcm5l/lpkXZ+bN5Rw3Bw05kj060lTCz0mqeTyjjcZpMdUciIHy7fotVENgHy9/D5RLVbwKtI70Oln+hupU+kTEBzPzsVl+G/uBdZk5WsLC/oh4PjN3R8ReYAulV6t4N3D4HEfj7KI6mdzuesAA9raG0lrzeyJiWWMIaCvVPKKohaORzNzY+B+Ha8/ZUdab2nvZBuyKiJ9n5pHafKLT61WeczgiVmTmkXGWXV/OaHXX1IeuyjKXAl+jOuVBva79ekTcWft7CXCs3I4Cv6caNv1R+fuYJ+qTDDrSfASgE1Snxj90jgZvCbCo/LmoNGytz9/60phNx4pmj0Ob+SGbW6GlBITDwHtKuNlZGvnltSDwSeCBc/zf4UbI2VaWv7G2Hkci4jZgO1APDHsbAeKB8pw3vLfa7w+XgLSuFsD2lNe9EzgCfK4se3dtHe6OiO1Upxu4e5xlN5czE06U3pc1jZ6ch6iGr05m5qt+kqTZ5dCVNPuB6NXMPFpuhzLz2XLbVx6f7oTRw5kZ9dsEXjMGLC//vxUK3lnrhVlRGv+zaQa0ZaU3pel3teWOZyLPmYjlwIbm0NUEtlEr3Jw3g9v9tczcmZkXAO+nOms3wP8r+4IhR5oD9uhIAhjm9eGrq6l6RY506XvZ22YIbL7D7j5gXzlh31J3N8mgI2luPUw1jwfgfUzyqKDiKGcOB7WcVxr7uQhOR4DLZ2hZY7MQeF6hOlJK0hxx6EoSZc7L4TLPZkN9jssk7IHTk4Ypvw9SzX25bQ4D24r6OpT1GJvisNjl7h1Sd7NHR+p+K9rMQ5nK8E1rQvDwFMPSkSpTREZE/WzNm6cYnKYU2CJiRQlt9XVYN4UepRvLcpJqHpQnZZS6kGdGlubzA+hFPd2ukmaVQ1eSJMmgI0mSZNCRJEky6EiSJBl0JEmSDDqSJEkGHUmSZNCRJEky6EiaqrGI8Iy7PaRsz2OWhGTQkQQHgEGLoaesKdtVkkFH6ns/A95rMfSUS4BfWAxSZ/BaV9J8fgAjlgIvAVdl5guWSNdvz1XAfuDCzDxqiUjzzx4daR5l5nHgZmA4IhZYIl0dchYA3wU+b8iRDDqSXg87e6jmdLwYEWsska4MOa2enLHM/LYlInXQ59OhK6ljGssh4F5gN/AccDAzxyyZjt1eK6kmHl8CXEPVk2PIkQw6ks7SeA4AW4B3UB2NtcRS6VjHqHrifgE85HCVZNCRJEmaU87RkSRJBh1JkiSDjiRJkkFHkiTJoCNJkmTQkSRJMuhIkiSDjiRJkkFHkiTJoCNJkmTQkSRJmrb/D6SCNQI+LjJzAAAAAElFTkSuQmCC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeneTRdyZDvy"
      },
      "source": [
        "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process:\n",
        "\n",
        "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
        "\n",
        "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3H88JXkI93v"
      },
      "source": [
        "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
        "\n",
        "state-action-reward are specified as follows:\n",
        "\n",
        "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
        "\n",
        "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
        "\n",
        "\n",
        "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKyZejI0fmp1"
      },
      "source": [
        "## Read data\n",
        "\n",
        "We first read the .csv file of our training data into dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_data.csv')\n",
        "\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure\n",
        "# it has the columns and index in the form that could be make into the environment.\n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw95ZMicgEyi"
      },
      "source": [
        "## Construct the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WZ6-9q2gq9S"
      },
      "source": [
        "Calculate and specify the parameters we need for constructing the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "a2e0a50d-3dc7-4dc2-ded2-15c691af5d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We-q73jjaFQ"
      },
      "source": [
        "## Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "4ed3d060-892d-4ad2-9563-09e3d19761c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# Part 3: Train DRL Agents\n",
        "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
        "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "d2f833c7-6a8a-4b6e-cc8c-980747e99ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "Logging to results/a2c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVpkWGqH4-D",
        "outputId": "58973a03-f55e-4810-fc23-678b4cd4b853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 180       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.315    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -64       |\n",
            "|    reward             | 0.6167687 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.91      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 198        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -93.6      |\n",
            "|    reward             | 0.08566034 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 6.69       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 200       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -191      |\n",
            "|    reward             | 11.297019 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 25.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 188        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | 78         |\n",
            "|    reward             | -1.1536155 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 86         |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 176        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0.00638    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 1.63e+03   |\n",
            "|    reward             | -25.444408 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.06e+03   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 170        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0.0876     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 171        |\n",
            "|    reward             | -0.5306599 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 18         |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 165        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 21         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0.365      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -108       |\n",
            "|    reward             | -2.6969163 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 8.18       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 162        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 24         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 171        |\n",
            "|    reward             | -1.6536961 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 17.6       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 160       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 133       |\n",
            "|    reward             | -2.601553 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 12.2      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 158       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -5.18     |\n",
            "|    reward             | -4.000884 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.38      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 157       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -442      |\n",
            "|    reward             | 4.088     |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 112       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 156       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.0189    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -111      |\n",
            "|    reward             | 0.2068664 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 9.44      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 154        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 41         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 9.71       |\n",
            "|    reward             | -3.8236427 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.75       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 154       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 20.7      |\n",
            "|    reward             | 1.5292113 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.72      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 153       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 48        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 214       |\n",
            "|    reward             | 4.1032233 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 33.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 152       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 52        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 104       |\n",
            "|    reward             | 1.7754455 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 7.26      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 152      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 55       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | -145     |\n",
            "|    reward             | 4.085998 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 97.8     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 151       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 59        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -43.2     |\n",
            "|    reward             | 1.7730774 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.19      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 151       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 62        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -28.1     |\n",
            "|    reward             | 0.8550308 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.39      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 151         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 66          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.6       |\n",
            "|    explained_variance | -0.0609     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -51.7       |\n",
            "|    reward             | -0.33453703 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 2.04        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 150       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 69        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 62.2      |\n",
            "|    reward             | 2.6180623 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.31      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 150      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 73       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | 148      |\n",
            "|    reward             | -8.18725 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 15.1     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 150        |\n",
            "|    iterations         | 2300       |\n",
            "|    time_elapsed       | 76         |\n",
            "|    total_timesteps    | 11500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2299       |\n",
            "|    policy_loss        | -2.62e+03  |\n",
            "|    reward             | -2.3568094 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.48e+03   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 149        |\n",
            "|    iterations         | 2400       |\n",
            "|    time_elapsed       | 80         |\n",
            "|    total_timesteps    | 12000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2399       |\n",
            "|    policy_loss        | 46.2       |\n",
            "|    reward             | 0.95126975 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 6.14       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 149        |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 83         |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | -48        |\n",
            "|    reward             | -1.0258087 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.66       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 149       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 86        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0.0659    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 10.9      |\n",
            "|    reward             | 2.1424882 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.775     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 149        |\n",
            "|    iterations         | 2700       |\n",
            "|    time_elapsed       | 90         |\n",
            "|    total_timesteps    | 13500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2699       |\n",
            "|    policy_loss        | -88.5      |\n",
            "|    reward             | -0.6733058 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.51       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 149       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 93        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 555       |\n",
            "|    reward             | 3.9937572 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 276       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 148       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 97        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -97.4     |\n",
            "|    reward             | 1.7695332 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 6.65      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 148       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 100       |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 17.3      |\n",
            "|    reward             | 0.5812935 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.757     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 148        |\n",
            "|    iterations         | 3100       |\n",
            "|    time_elapsed       | 104        |\n",
            "|    total_timesteps    | 15500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3099       |\n",
            "|    policy_loss        | 105        |\n",
            "|    reward             | 0.06572997 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 9.02       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 148        |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 107        |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | -128       |\n",
            "|    reward             | -3.0572078 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 35.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 148       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 111       |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 7.08      |\n",
            "|    reward             | -3.965273 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.91      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 148      |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 114      |\n",
            "|    total_timesteps    | 17000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | 498      |\n",
            "|    reward             | 8.893025 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 184      |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 149         |\n",
            "|    iterations         | 3500        |\n",
            "|    time_elapsed       | 117         |\n",
            "|    total_timesteps    | 17500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.6       |\n",
            "|    explained_variance | -0.0137     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3499        |\n",
            "|    policy_loss        | 165         |\n",
            "|    reward             | -0.19183353 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 22.4        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 149       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 120       |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -12.7     |\n",
            "|    reward             | 2.2294774 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.25      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 150       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 123       |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 179       |\n",
            "|    reward             | 10.331943 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 27.9      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 150       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 126       |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | -135      |\n",
            "|    reward             | 7.5026894 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 14        |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 151       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 129       |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | -0.000761 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -104      |\n",
            "|    reward             | 2.9485104 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 12.2      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 151      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 132      |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -905     |\n",
            "|    reward             | 3.93376  |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 471      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 151        |\n",
            "|    iterations         | 4100       |\n",
            "|    time_elapsed       | 135        |\n",
            "|    total_timesteps    | 20500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | -0.00081   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4099       |\n",
            "|    policy_loss        | 23.8       |\n",
            "|    reward             | -0.5295016 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.4        |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 152      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 138      |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | -159     |\n",
            "|    reward             | 1.162438 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 22.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 152       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 141       |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | -254      |\n",
            "|    reward             | 5.7925253 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 34.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 152       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 144       |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 38.8      |\n",
            "|    reward             | 0.5499902 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 33.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 153        |\n",
            "|    iterations         | 4500       |\n",
            "|    time_elapsed       | 147        |\n",
            "|    total_timesteps    | 22500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4499       |\n",
            "|    policy_loss        | 1.96e+03   |\n",
            "|    reward             | 0.19892246 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.12e+03   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 153      |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 150      |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 544      |\n",
            "|    reward             | 8.022137 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 228      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 153       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 153       |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | -97.2     |\n",
            "|    reward             | 1.3895634 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 10.8      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 153        |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 156        |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -142       |\n",
            "|    reward             | -0.3093836 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 12.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 154       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 159       |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -17.7     |\n",
            "|    reward             | 1.7918288 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.51      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 154       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 162       |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | -225      |\n",
            "|    reward             | 1.6271389 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 37.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 154       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 165       |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -305      |\n",
            "|    reward             | 13.916371 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 81.9      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 154       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 168       |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -994      |\n",
            "|    reward             | 10.531571 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 581       |\n",
            "-------------------------------------\n",
            "day: 2892, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 11000979.74\n",
            "total_reward: 10000979.74\n",
            "total_cost: 8523.76\n",
            "total_trades: 51602\n",
            "Sharpe: 1.085\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 154        |\n",
            "|    iterations         | 5300       |\n",
            "|    time_elapsed       | 171        |\n",
            "|    total_timesteps    | 26500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 2.32e-06   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5299       |\n",
            "|    policy_loss        | -101       |\n",
            "|    reward             | -0.5480674 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 6.23       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 154         |\n",
            "|    iterations         | 5400        |\n",
            "|    time_elapsed       | 174         |\n",
            "|    total_timesteps    | 27000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5399        |\n",
            "|    policy_loss        | -108        |\n",
            "|    reward             | 0.039967578 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 7.15        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 154      |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 177      |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | 372      |\n",
            "|    reward             | 4.122642 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 92       |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 155       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 180       |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | -196      |\n",
            "|    reward             | 2.9972327 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 31.4      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 155      |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 183      |\n",
            "|    total_timesteps    | 28500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | -23.9    |\n",
            "|    reward             | 1.126826 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 5.81     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 155       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 186       |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | -1.86     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 38.5      |\n",
            "|    reward             | 2.3358815 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 12.7      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 155         |\n",
            "|    iterations         | 5900        |\n",
            "|    time_elapsed       | 189         |\n",
            "|    total_timesteps    | 29500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5899        |\n",
            "|    policy_loss        | 105         |\n",
            "|    reward             | -0.93226105 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 9.09        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 155        |\n",
            "|    iterations         | 6000       |\n",
            "|    time_elapsed       | 192        |\n",
            "|    total_timesteps    | 30000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5999       |\n",
            "|    policy_loss        | 105        |\n",
            "|    reward             | 0.07558769 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 7.14       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 155       |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 195       |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | -94.6     |\n",
            "|    reward             | -5.312141 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 8.61      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 155        |\n",
            "|    iterations         | 6200       |\n",
            "|    time_elapsed       | 198        |\n",
            "|    total_timesteps    | 31000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6199       |\n",
            "|    policy_loss        | 48.3       |\n",
            "|    reward             | -0.2643229 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 7.21       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 156       |\n",
            "|    iterations         | 6300      |\n",
            "|    time_elapsed       | 201       |\n",
            "|    total_timesteps    | 31500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6299      |\n",
            "|    policy_loss        | 532       |\n",
            "|    reward             | 4.9138722 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 245       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 156       |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 204       |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | -0.0632   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 61.3      |\n",
            "|    reward             | 2.6271725 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.69      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 156       |\n",
            "|    iterations         | 6500      |\n",
            "|    time_elapsed       | 207       |\n",
            "|    total_timesteps    | 32500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6499      |\n",
            "|    policy_loss        | 9.66      |\n",
            "|    reward             | -4.217664 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 5.54      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 156        |\n",
            "|    iterations         | 6600       |\n",
            "|    time_elapsed       | 210        |\n",
            "|    total_timesteps    | 33000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6599       |\n",
            "|    policy_loss        | -22.6      |\n",
            "|    reward             | -0.9152231 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 3.25       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 156       |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 213       |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | -693      |\n",
            "|    reward             | -6.238579 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 304       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 156       |\n",
            "|    iterations         | 6800      |\n",
            "|    time_elapsed       | 216       |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | -134      |\n",
            "|    reward             | 1.4551866 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 12.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 157        |\n",
            "|    iterations         | 6900       |\n",
            "|    time_elapsed       | 219        |\n",
            "|    total_timesteps    | 34500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | -0.000953  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6899       |\n",
            "|    policy_loss        | -38.5      |\n",
            "|    reward             | 0.31177253 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 12.7       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 157        |\n",
            "|    iterations         | 7000       |\n",
            "|    time_elapsed       | 222        |\n",
            "|    total_timesteps    | 35000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6999       |\n",
            "|    policy_loss        | 63.9       |\n",
            "|    reward             | -0.9889828 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 2.61       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 157       |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 225       |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | -47.5     |\n",
            "|    reward             | 1.8652327 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 3.85      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 157        |\n",
            "|    iterations         | 7200       |\n",
            "|    time_elapsed       | 228        |\n",
            "|    total_timesteps    | 36000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7199       |\n",
            "|    policy_loss        | -309       |\n",
            "|    reward             | -1.8267448 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 62.7       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 157      |\n",
            "|    iterations         | 7300     |\n",
            "|    time_elapsed       | 231      |\n",
            "|    total_timesteps    | 36500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7299     |\n",
            "|    policy_loss        | -163     |\n",
            "|    reward             | 3.201204 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 23.2     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 157        |\n",
            "|    iterations         | 7400       |\n",
            "|    time_elapsed       | 235        |\n",
            "|    total_timesteps    | 37000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7399       |\n",
            "|    policy_loss        | 657        |\n",
            "|    reward             | -32.439133 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 389        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 157        |\n",
            "|    iterations         | 7500       |\n",
            "|    time_elapsed       | 238        |\n",
            "|    total_timesteps    | 37500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7499       |\n",
            "|    policy_loss        | 730        |\n",
            "|    reward             | -25.494081 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 564        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 157        |\n",
            "|    iterations         | 7600       |\n",
            "|    time_elapsed       | 241        |\n",
            "|    total_timesteps    | 38000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 5.96e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7599       |\n",
            "|    policy_loss        | -151       |\n",
            "|    reward             | 0.75522625 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 14.5       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 157       |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 244       |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | -0.653    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | -69.2     |\n",
            "|    reward             | 2.0976667 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 3.61      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 157         |\n",
            "|    iterations         | 7800        |\n",
            "|    time_elapsed       | 248         |\n",
            "|    total_timesteps    | 39000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7799        |\n",
            "|    policy_loss        | -19         |\n",
            "|    reward             | -0.80269676 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 3.23        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 156      |\n",
            "|    iterations         | 7900     |\n",
            "|    time_elapsed       | 251      |\n",
            "|    total_timesteps    | 39500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.1    |\n",
            "|    explained_variance | 0.0334   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7899     |\n",
            "|    policy_loss        | 385      |\n",
            "|    reward             | 6.003466 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 106      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 156        |\n",
            "|    iterations         | 8000       |\n",
            "|    time_elapsed       | 255        |\n",
            "|    total_timesteps    | 40000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7999       |\n",
            "|    policy_loss        | -178       |\n",
            "|    reward             | -1.3285459 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 33.4       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 156       |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 258       |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | -0.00175  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | 14.7      |\n",
            "|    reward             | 10.394028 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 21.4      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 156         |\n",
            "|    iterations         | 8200        |\n",
            "|    time_elapsed       | 262         |\n",
            "|    total_timesteps    | 41000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8199        |\n",
            "|    policy_loss        | -158        |\n",
            "|    reward             | -0.78462446 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 15.5        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 156       |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 265       |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | 69.6      |\n",
            "|    reward             | -2.015772 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 2.95      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 156        |\n",
            "|    iterations         | 8400       |\n",
            "|    time_elapsed       | 268        |\n",
            "|    total_timesteps    | 42000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8399       |\n",
            "|    policy_loss        | -98.8      |\n",
            "|    reward             | -1.3334053 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 8.85       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 156        |\n",
            "|    iterations         | 8500       |\n",
            "|    time_elapsed       | 272        |\n",
            "|    total_timesteps    | 42500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8499       |\n",
            "|    policy_loss        | -199       |\n",
            "|    reward             | -1.5177871 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 27.2       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 156        |\n",
            "|    iterations         | 8600       |\n",
            "|    time_elapsed       | 275        |\n",
            "|    total_timesteps    | 43000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8599       |\n",
            "|    policy_loss        | 181        |\n",
            "|    reward             | -2.1226468 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 38.9       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 155       |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 279       |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 0.0968    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | 7.56      |\n",
            "|    reward             | 1.9729284 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.71      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 155       |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 282       |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | -0.532    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | -34.6     |\n",
            "|    reward             | 0.4817786 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.3       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 155         |\n",
            "|    iterations         | 8900        |\n",
            "|    time_elapsed       | 285         |\n",
            "|    total_timesteps    | 44500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8899        |\n",
            "|    policy_loss        | 107         |\n",
            "|    reward             | -0.47404563 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 6.65        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 155         |\n",
            "|    iterations         | 9000        |\n",
            "|    time_elapsed       | 288         |\n",
            "|    total_timesteps    | 45000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.4       |\n",
            "|    explained_variance | -0.979      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8999        |\n",
            "|    policy_loss        | 12          |\n",
            "|    reward             | -0.91845256 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 5.14        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 155       |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 292       |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 144       |\n",
            "|    reward             | 1.9478127 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 16.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 155       |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 295       |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | 22.5      |\n",
            "|    reward             | 6.3785124 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 14.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 155       |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 298       |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | -3.7e-06  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | -145      |\n",
            "|    reward             | 1.4092417 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 13.1      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 155         |\n",
            "|    iterations         | 9400        |\n",
            "|    time_elapsed       | 302         |\n",
            "|    total_timesteps    | 47000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9399        |\n",
            "|    policy_loss        | 257         |\n",
            "|    reward             | -0.59443945 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 38.1        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 155       |\n",
            "|    iterations         | 9500      |\n",
            "|    time_elapsed       | 305       |\n",
            "|    total_timesteps    | 47500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9499      |\n",
            "|    policy_loss        | -56.7     |\n",
            "|    reward             | 0.6240898 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 2.83      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 155        |\n",
            "|    iterations         | 9600       |\n",
            "|    time_elapsed       | 308        |\n",
            "|    total_timesteps    | 48000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9599       |\n",
            "|    policy_loss        | 39         |\n",
            "|    reward             | -3.5204797 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 18.3       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 155      |\n",
            "|    iterations         | 9700     |\n",
            "|    time_elapsed       | 311      |\n",
            "|    total_timesteps    | 48500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9699     |\n",
            "|    policy_loss        | -68.4    |\n",
            "|    reward             | 1.543733 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 7.42     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 155       |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 314       |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | 245       |\n",
            "|    reward             | -3.39621  |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 54        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 155        |\n",
            "|    iterations         | 9900       |\n",
            "|    time_elapsed       | 317        |\n",
            "|    total_timesteps    | 49500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9899       |\n",
            "|    policy_loss        | 46.8       |\n",
            "|    reward             | 0.16808914 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 1.84       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 155        |\n",
            "|    iterations         | 10000      |\n",
            "|    time_elapsed       | 321        |\n",
            "|    total_timesteps    | 50000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0.00496    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9999       |\n",
            "|    policy_loss        | 72.2       |\n",
            "|    reward             | 0.11493963 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 4.46       |\n",
            "--------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c,\n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=50000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "outputs": [],
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M2YadjfnLwgt",
        "outputId": "3abbef24-c320-41e7-e97e-51329bbdeafa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tCDa78rqfO_a",
        "outputId": "a24554a0-d6d4-4c64-cd90-0737ba666459",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day: 2892, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6091841.90\n",
            "total_reward: 5091841.90\n",
            "total_cost: 4428.23\n",
            "total_trades: 61407\n",
            "Sharpe: 0.983\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 120       |\n",
            "|    time_elapsed    | 95        |\n",
            "|    total_timesteps | 11572     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 12        |\n",
            "|    critic_loss     | 50.3      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 11471     |\n",
            "|    reward          | 4.4920583 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 121       |\n",
            "|    time_elapsed    | 190       |\n",
            "|    total_timesteps | 23144     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.85     |\n",
            "|    critic_loss     | 15.2      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 23043     |\n",
            "|    reward          | 4.4920583 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5462843.45\n",
            "total_reward: 4462843.45\n",
            "total_cost: 999.00\n",
            "total_trades: 60732\n",
            "Sharpe: 0.864\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 121       |\n",
            "|    time_elapsed    | 285       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.46     |\n",
            "|    critic_loss     | 7.53      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | 4.4920583 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 122       |\n",
            "|    time_elapsed    | 378       |\n",
            "|    total_timesteps | 46288     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -10.4     |\n",
            "|    critic_loss     | 5.61      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 46187     |\n",
            "|    reward          | 4.4920583 |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg,\n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=50000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ne6M2R-WvrUQ"
      },
      "outputs": [],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y5D5PFUhMzSV",
        "outputId": "443e7809-63a3-4b35-f2b8-37eee4671ca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cuda device\n",
            "Logging to results/ppo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Gt8eIQKYM4G3",
        "outputId": "727c2f06-ba81-46be-8426-47e2bfd42801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 167         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 12          |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | 0.116336495 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 165         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 24          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015823577 |\n",
            "|    clip_fraction        | 0.212       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.0111      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.88        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0254     |\n",
            "|    reward               | 0.7071445   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 11          |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 166          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0146554075 |\n",
            "|    clip_fraction        | 0.206        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -41.2        |\n",
            "|    explained_variance   | 0.00126      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 30.5         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.0164      |\n",
            "|    reward               | -1.3482705   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 57.3         |\n",
            "------------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 167       |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 48        |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0205783 |\n",
            "|    clip_fraction        | 0.213     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -41.2     |\n",
            "|    explained_variance   | -0.0117   |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 16        |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | -0.0177   |\n",
            "|    reward               | 2.348878  |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 88.9      |\n",
            "---------------------------------------\n",
            "day: 2892, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3880656.23\n",
            "total_reward: 2880656.23\n",
            "total_cost: 316518.99\n",
            "total_trades: 80064\n",
            "Sharpe: 0.748\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 167         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 61          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015208868 |\n",
            "|    clip_fraction        | 0.171       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0165     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.7        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0219     |\n",
            "|    reward               | 2.2965598   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 22.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 168         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 72          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019027695 |\n",
            "|    clip_fraction        | 0.233       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.000566    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 61.1        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0211     |\n",
            "|    reward               | 1.8395213   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 95          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 169         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 84          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010270461 |\n",
            "|    clip_fraction        | 0.0907      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.0038      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 117         |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    reward               | 2.4594257   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 226         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 170         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 95          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025501844 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.00357    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14.8        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    reward               | 0.058874644 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 32.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 171         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 107         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018961668 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | 0.000713    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 34.9        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0198     |\n",
            "|    reward               | 0.49660614  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 65.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 172         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 118         |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013673605 |\n",
            "|    clip_fraction        | 0.0809      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | -0.00579    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 307         |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    reward               | 0.9790222   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 382         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 172         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 130         |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027429666 |\n",
            "|    clip_fraction        | 0.246       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | 0.00842     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 67.2        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0172     |\n",
            "|    reward               | 1.984139    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 81          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 173         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 141         |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023668226 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | 0.0132      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.5         |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0161     |\n",
            "|    reward               | 0.1787447   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 12.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 173         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 153         |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024824977 |\n",
            "|    clip_fraction        | 0.224       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.0279     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 28.9        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0205     |\n",
            "|    reward               | 0.2875563   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 54.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 173         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 164         |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023888929 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.0213     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18          |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    reward               | -0.73778725 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 48.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 174         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 176         |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028098328 |\n",
            "|    clip_fraction        | 0.287       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.00505    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.6        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0113     |\n",
            "|    reward               | 4.8329725   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 25.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 174         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 187         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028042583 |\n",
            "|    clip_fraction        | 0.283       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | -0.00278    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27.4        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0152     |\n",
            "|    reward               | 0.4318725   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 43.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 174         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 199         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028119348 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | -0.0211     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14          |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0126     |\n",
            "|    reward               | 0.65427065  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 80.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 174         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 211         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020838045 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | -0.0154     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 15.2        |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.013      |\n",
            "|    reward               | 0.341382    |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 49.9        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6996416.49\n",
            "total_reward: 5996416.49\n",
            "total_cost: 261108.95\n",
            "total_trades: 76328\n",
            "Sharpe: 1.103\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 174         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 223         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031406373 |\n",
            "|    clip_fraction        | 0.272       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | 0.0284      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.58        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.00725    |\n",
            "|    reward               | -0.56651306 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 16.7        |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 173       |\n",
            "|    iterations           | 20        |\n",
            "|    time_elapsed         | 235       |\n",
            "|    total_timesteps      | 40960     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0211595 |\n",
            "|    clip_fraction        | 0.209     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -42.1     |\n",
            "|    explained_variance   | 0.031     |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 84.5      |\n",
            "|    n_updates            | 190       |\n",
            "|    policy_gradient_loss | -0.0139   |\n",
            "|    reward               | 0.1469968 |\n",
            "|    std                  | 1.03      |\n",
            "|    value_loss           | 124       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 172        |\n",
            "|    iterations           | 21         |\n",
            "|    time_elapsed         | 249        |\n",
            "|    total_timesteps      | 43008      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01749644 |\n",
            "|    clip_fraction        | 0.189      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.1      |\n",
            "|    explained_variance   | 0.0355     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 77         |\n",
            "|    n_updates            | 200        |\n",
            "|    policy_gradient_loss | -0.0131    |\n",
            "|    reward               | -5.1837373 |\n",
            "|    std                  | 1.04       |\n",
            "|    value_loss           | 96.8       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 172         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 261         |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022026006 |\n",
            "|    clip_fraction        | 0.224       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.00133     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.1         |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0127     |\n",
            "|    reward               | 3.011774    |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 13.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 172         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 273         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023589928 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.0182      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 25.9        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    reward               | -0.25613216 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 36.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 172         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 285         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021916918 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | -0.00301    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 72.2        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0118     |\n",
            "|    reward               | 9.635426    |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 82.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 171         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 298         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.043107927 |\n",
            "|    clip_fraction        | 0.315       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | 0.0289      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 77.4        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.00173    |\n",
            "|    reward               | -0.6869938  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 105         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 171         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 310         |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024026357 |\n",
            "|    clip_fraction        | 0.288       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | 0.182       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.76        |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.00972    |\n",
            "|    reward               | -0.5640934  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 15.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 171         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 322         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028070552 |\n",
            "|    clip_fraction        | 0.269       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.0567      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 29.5        |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00565    |\n",
            "|    reward               | -1.1818792  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 61.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 171         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 334         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020486597 |\n",
            "|    clip_fraction        | 0.224       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.0452      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 49.8        |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.00919    |\n",
            "|    reward               | -2.1518743  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 70.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 170         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 347         |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025659714 |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | 0.00842     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.4        |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.00565    |\n",
            "|    reward               | 0.39084995  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 22.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 170         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 360         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025947306 |\n",
            "|    clip_fraction        | 0.222       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | 0.0501      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 53.1        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.00806    |\n",
            "|    reward               | 1.1986313   |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 100         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 170         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 373         |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033551734 |\n",
            "|    clip_fraction        | 0.327       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | 0.14        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 35.1        |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.00234    |\n",
            "|    reward               | 3.2544038   |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 60.5        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 169        |\n",
            "|    iterations           | 32         |\n",
            "|    time_elapsed         | 386        |\n",
            "|    total_timesteps      | 65536      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02440321 |\n",
            "|    clip_fraction        | 0.228      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.6      |\n",
            "|    explained_variance   | 0.115      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 20.2       |\n",
            "|    n_updates            | 310        |\n",
            "|    policy_gradient_loss | -0.00869   |\n",
            "|    reward               | -0.1282071 |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 35.1       |\n",
            "----------------------------------------\n",
            "day: 2892, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6024651.40\n",
            "total_reward: 5024651.40\n",
            "total_cost: 273671.59\n",
            "total_trades: 77708\n",
            "Sharpe: 1.031\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 169         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 399         |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031783238 |\n",
            "|    clip_fraction        | 0.237       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.0295      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.6        |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.0113     |\n",
            "|    reward               | -0.42703474 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 79.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 169         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 411         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026604105 |\n",
            "|    clip_fraction        | 0.211       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.157       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 107         |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    reward               | 1.0567305   |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 99.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 169         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 423         |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017678544 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | 0.0487      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 75.1        |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.00662    |\n",
            "|    reward               | -0.9619967  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 154         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 168        |\n",
            "|    iterations           | 36         |\n",
            "|    time_elapsed         | 437        |\n",
            "|    total_timesteps      | 73728      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01796458 |\n",
            "|    clip_fraction        | 0.178      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.8      |\n",
            "|    explained_variance   | 0.0415     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 15.7       |\n",
            "|    n_updates            | 350        |\n",
            "|    policy_gradient_loss | -0.00626   |\n",
            "|    reward               | -5.4664907 |\n",
            "|    std                  | 1.06       |\n",
            "|    value_loss           | 27.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 168         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 448         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020895902 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | 0.000764    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 47.6        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.0118     |\n",
            "|    reward               | -0.49910036 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 131         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 168         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 461         |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018631782 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | -0.0084     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 101         |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.00556    |\n",
            "|    reward               | -8.211165   |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 111         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 168         |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 475         |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018628256 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | -0.0526     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 12.9        |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.00918    |\n",
            "|    reward               | -3.0607696  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 35.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 167        |\n",
            "|    iterations           | 40         |\n",
            "|    time_elapsed         | 488        |\n",
            "|    total_timesteps      | 81920      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02111035 |\n",
            "|    clip_fraction        | 0.182      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.9      |\n",
            "|    explained_variance   | 0.00925    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 48.1       |\n",
            "|    n_updates            | 390        |\n",
            "|    policy_gradient_loss | -0.00945   |\n",
            "|    reward               | -1.1056489 |\n",
            "|    std                  | 1.06       |\n",
            "|    value_loss           | 94.5       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 167         |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 501         |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019191884 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | 0.0787      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 73          |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.0126     |\n",
            "|    reward               | 0.7103277   |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 117         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 167         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 514         |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023052508 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | -0.0538     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 70.6        |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.00637    |\n",
            "|    reward               | -0.32067227 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 144         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 166         |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 527         |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025730658 |\n",
            "|    clip_fraction        | 0.271       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | -0.0317     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.77        |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.016      |\n",
            "|    reward               | -1.8920592  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 22.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 166         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 540         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023699544 |\n",
            "|    clip_fraction        | 0.249       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | 0.0608      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 43.9        |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.0126     |\n",
            "|    reward               | 0.22531292  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 82.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 166         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 553         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025001518 |\n",
            "|    clip_fraction        | 0.245       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | 0.0272      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 54.8        |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.00911    |\n",
            "|    reward               | -0.6260161  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 112         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 166         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 566         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032809332 |\n",
            "|    clip_fraction        | 0.254       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.0397      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.71        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.00954    |\n",
            "|    reward               | -5.0042715  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 15.1        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4563386.30\n",
            "total_reward: 3563386.30\n",
            "total_cost: 269950.86\n",
            "total_trades: 77568\n",
            "Sharpe: 0.897\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 166         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 579         |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029484298 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.0201      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 46.6        |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    reward               | 1.7421345   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 55          |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 165        |\n",
            "|    iterations           | 48         |\n",
            "|    time_elapsed         | 592        |\n",
            "|    total_timesteps      | 98304      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02606561 |\n",
            "|    clip_fraction        | 0.222      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.2      |\n",
            "|    explained_variance   | -0.00474   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 31.6       |\n",
            "|    n_updates            | 470        |\n",
            "|    policy_gradient_loss | -0.0105    |\n",
            "|    reward               | 12.335075  |\n",
            "|    std                  | 1.07       |\n",
            "|    value_loss           | 67.1       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 165         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 605         |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027164914 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | -0.0257     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 50.1        |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.00815    |\n",
            "|    reward               | 0.6426058   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 93.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 165         |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 618         |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024290374 |\n",
            "|    clip_fraction        | 0.258       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.126       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.86        |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | -0.0184     |\n",
            "|    reward               | -0.7253005  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 22.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 165         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 631         |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.037671745 |\n",
            "|    clip_fraction        | 0.269       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.0586      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 63.2        |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.00434    |\n",
            "|    reward               | -0.19553618 |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 75.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 165         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 644         |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031123677 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.4       |\n",
            "|    explained_variance   | 0.0358      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 48.5        |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.00346    |\n",
            "|    reward               | -5.279248   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 83.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 165         |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 657         |\n",
            "|    total_timesteps      | 108544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.038470153 |\n",
            "|    clip_fraction        | 0.249       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.181       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 11.9        |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    reward               | 1.8544718   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 20.7        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 164        |\n",
            "|    iterations           | 54         |\n",
            "|    time_elapsed         | 670        |\n",
            "|    total_timesteps      | 110592     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02929826 |\n",
            "|    clip_fraction        | 0.23       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.5      |\n",
            "|    explained_variance   | 0.0295     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 15.8       |\n",
            "|    n_updates            | 530        |\n",
            "|    policy_gradient_loss | -0.00743   |\n",
            "|    reward               | 1.2578886  |\n",
            "|    std                  | 1.09       |\n",
            "|    value_loss           | 56.8       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 164        |\n",
            "|    iterations           | 55         |\n",
            "|    time_elapsed         | 683        |\n",
            "|    total_timesteps      | 112640     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03057673 |\n",
            "|    clip_fraction        | 0.304      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.6      |\n",
            "|    explained_variance   | 0.0497     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 44.2       |\n",
            "|    n_updates            | 540        |\n",
            "|    policy_gradient_loss | -0.00612   |\n",
            "|    reward               | 5.0154414  |\n",
            "|    std                  | 1.09       |\n",
            "|    value_loss           | 54.5       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 164         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 696         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.054078393 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.7       |\n",
            "|    explained_variance   | 0.158       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.03        |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.00462    |\n",
            "|    reward               | 0.5622713   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 23.3        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 164          |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 709          |\n",
            "|    total_timesteps      | 116736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.02942545   |\n",
            "|    clip_fraction        | 0.259        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -43.7        |\n",
            "|    explained_variance   | 0.0343       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 13.5         |\n",
            "|    n_updates            | 560          |\n",
            "|    policy_gradient_loss | -0.0139      |\n",
            "|    reward               | -0.043956835 |\n",
            "|    std                  | 1.09         |\n",
            "|    value_loss           | 47.2         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 164         |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 722         |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.048267562 |\n",
            "|    clip_fraction        | 0.32        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.7       |\n",
            "|    explained_variance   | 0.0102      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 39.5        |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.00284    |\n",
            "|    reward               | 1.4115323   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 59.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 164         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 735         |\n",
            "|    total_timesteps      | 120832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029447597 |\n",
            "|    clip_fraction        | 0.332       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.8       |\n",
            "|    explained_variance   | -0.00489    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 54.6        |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.00764    |\n",
            "|    reward               | -1.1207227  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 51.7        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 164        |\n",
            "|    iterations           | 60         |\n",
            "|    time_elapsed         | 748        |\n",
            "|    total_timesteps      | 122880     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06267566 |\n",
            "|    clip_fraction        | 0.325      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.8      |\n",
            "|    explained_variance   | 0.247      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 10         |\n",
            "|    n_updates            | 590        |\n",
            "|    policy_gradient_loss | -0.00754   |\n",
            "|    reward               | 0.35041258 |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 22.7       |\n",
            "----------------------------------------\n",
            "day: 2892, episode: 80\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5044559.03\n",
            "total_reward: 4044559.03\n",
            "total_cost: 259109.78\n",
            "total_trades: 76741\n",
            "Sharpe: 0.914\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 163        |\n",
            "|    iterations           | 61         |\n",
            "|    time_elapsed         | 763        |\n",
            "|    total_timesteps      | 124928     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03775368 |\n",
            "|    clip_fraction        | 0.302      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.9      |\n",
            "|    explained_variance   | 0.117      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 37.6       |\n",
            "|    n_updates            | 600        |\n",
            "|    policy_gradient_loss | -0.00588   |\n",
            "|    reward               | 1.4606855  |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 121        |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 163          |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 776          |\n",
            "|    total_timesteps      | 126976       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.030601822  |\n",
            "|    clip_fraction        | 0.245        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -44          |\n",
            "|    explained_variance   | 0.0997       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 24.8         |\n",
            "|    n_updates            | 610          |\n",
            "|    policy_gradient_loss | -0.00772     |\n",
            "|    reward               | -0.055990364 |\n",
            "|    std                  | 1.1          |\n",
            "|    value_loss           | 81.1         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 163         |\n",
            "|    iterations           | 63          |\n",
            "|    time_elapsed         | 789         |\n",
            "|    total_timesteps      | 129024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025555387 |\n",
            "|    clip_fraction        | 0.242       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.11        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14.8        |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | -0.00432    |\n",
            "|    reward               | 4.104953    |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 34.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 163         |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 802         |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031053193 |\n",
            "|    clip_fraction        | 0.265       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | 0.175       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 34.7        |\n",
            "|    n_updates            | 630         |\n",
            "|    policy_gradient_loss | -0.00364    |\n",
            "|    reward               | -0.3632135  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 72.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 163         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 814         |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032007627 |\n",
            "|    clip_fraction        | 0.31        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.097       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 55.5        |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | -0.000592   |\n",
            "|    reward               | -1.0395263  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 98.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 163         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 827         |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029013868 |\n",
            "|    clip_fraction        | 0.357       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.0694      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 35.7        |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | 0.000856    |\n",
            "|    reward               | 2.9459956   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 67.3        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 163        |\n",
            "|    iterations           | 67         |\n",
            "|    time_elapsed         | 840        |\n",
            "|    total_timesteps      | 137216     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03031492 |\n",
            "|    clip_fraction        | 0.279      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.5      |\n",
            "|    explained_variance   | 0.191      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 5.95       |\n",
            "|    n_updates            | 660        |\n",
            "|    policy_gradient_loss | -0.000945  |\n",
            "|    reward               | -1.7215502 |\n",
            "|    std                  | 1.12       |\n",
            "|    value_loss           | 13.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 163         |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 853         |\n",
            "|    total_timesteps      | 139264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023257157 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.111       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 42.3        |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | -0.0026     |\n",
            "|    reward               | -0.25259432 |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 87.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 163         |\n",
            "|    iterations           | 69          |\n",
            "|    time_elapsed         | 866         |\n",
            "|    total_timesteps      | 141312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024987692 |\n",
            "|    clip_fraction        | 0.249       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.131       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 59.4        |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | -0.00356    |\n",
            "|    reward               | -2.073239   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 105         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 879         |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023727454 |\n",
            "|    clip_fraction        | 0.234       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.0321      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.87        |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | -0.00475    |\n",
            "|    reward               | 0.118560314 |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 24.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 71          |\n",
            "|    time_elapsed         | 893         |\n",
            "|    total_timesteps      | 145408      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016429922 |\n",
            "|    clip_fraction        | 0.14        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.092       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.6        |\n",
            "|    n_updates            | 700         |\n",
            "|    policy_gradient_loss | -0.0107     |\n",
            "|    reward               | 0.58162534  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 103         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 162        |\n",
            "|    iterations           | 72         |\n",
            "|    time_elapsed         | 906        |\n",
            "|    total_timesteps      | 147456     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02008964 |\n",
            "|    clip_fraction        | 0.217      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.6      |\n",
            "|    explained_variance   | 0.0861     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 22.6       |\n",
            "|    n_updates            | 710        |\n",
            "|    policy_gradient_loss | -0.00284   |\n",
            "|    reward               | -39.957283 |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 84         |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 919         |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026185546 |\n",
            "|    clip_fraction        | 0.305       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.114       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 20.4        |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | 0.0017      |\n",
            "|    reward               | -2.1101055  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 76.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 932         |\n",
            "|    total_timesteps      | 151552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019176947 |\n",
            "|    clip_fraction        | 0.157       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.196       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.7        |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | -0.0125     |\n",
            "|    reward               | -1.9185346  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 76.4        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 90\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 7182209.79\n",
            "total_reward: 6182209.79\n",
            "total_cost: 214880.29\n",
            "total_trades: 72268\n",
            "Sharpe: 1.080\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 162        |\n",
            "|    iterations           | 75         |\n",
            "|    time_elapsed         | 945        |\n",
            "|    total_timesteps      | 153600     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04053115 |\n",
            "|    clip_fraction        | 0.313      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.7      |\n",
            "|    explained_variance   | 0.269      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 13.5       |\n",
            "|    n_updates            | 740        |\n",
            "|    policy_gradient_loss | -0.00353   |\n",
            "|    reward               | 2.3423505  |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 109        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 958         |\n",
            "|    total_timesteps      | 155648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021788921 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.194       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 70.2        |\n",
            "|    n_updates            | 750         |\n",
            "|    policy_gradient_loss | -0.00525    |\n",
            "|    reward               | -2.4992495  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 143         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 971         |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030361341 |\n",
            "|    clip_fraction        | 0.264       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | 0.44        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.2        |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | 0.000369    |\n",
            "|    reward               | 0.91915417  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 24.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 78          |\n",
            "|    time_elapsed         | 984         |\n",
            "|    total_timesteps      | 159744      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027919913 |\n",
            "|    clip_fraction        | 0.265       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | 0.35        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 115         |\n",
            "|    n_updates            | 770         |\n",
            "|    policy_gradient_loss | -0.00808    |\n",
            "|    reward               | 2.0611205   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 145         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 998         |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030632941 |\n",
            "|    clip_fraction        | 0.184       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.199       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 45.3        |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | -0.00682    |\n",
            "|    reward               | -3.8404894  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 185         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 1010        |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035841063 |\n",
            "|    clip_fraction        | 0.372       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.363       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.1        |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | 0.00315     |\n",
            "|    reward               | 0.120592326 |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 44.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 1024        |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022759188 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.276       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 76.9        |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.0136     |\n",
            "|    reward               | -0.12256193 |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 119         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 82          |\n",
            "|    time_elapsed         | 1037        |\n",
            "|    total_timesteps      | 167936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015212083 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | 0.0246      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 109         |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | -0.00641    |\n",
            "|    reward               | -1.0883794  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 135         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 83          |\n",
            "|    time_elapsed         | 1050        |\n",
            "|    total_timesteps      | 169984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031805027 |\n",
            "|    clip_fraction        | 0.241       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | 0.336       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 52.2        |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | -0.0032     |\n",
            "|    reward               | -0.8041815  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 116         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 84          |\n",
            "|    time_elapsed         | 1063        |\n",
            "|    total_timesteps      | 172032      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034690604 |\n",
            "|    clip_fraction        | 0.343       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | 0.634       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.72        |\n",
            "|    n_updates            | 830         |\n",
            "|    policy_gradient_loss | -0.00253    |\n",
            "|    reward               | 0.9707521   |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 21.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 1075        |\n",
            "|    total_timesteps      | 174080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023352271 |\n",
            "|    clip_fraction        | 0.221       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.2       |\n",
            "|    explained_variance   | 0.29        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 67.7        |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | 0.00194     |\n",
            "|    reward               | -0.2528363  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 145         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 1088        |\n",
            "|    total_timesteps      | 176128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033353925 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.2       |\n",
            "|    explained_variance   | 0.245       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.1        |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | -0.00892    |\n",
            "|    reward               | 0.16381557  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 177         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 1101        |\n",
            "|    total_timesteps      | 178176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016926192 |\n",
            "|    clip_fraction        | 0.22        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.3       |\n",
            "|    explained_variance   | 0.501       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.4        |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | 0.00392     |\n",
            "|    reward               | 2.9212604   |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 27.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 1114        |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035010643 |\n",
            "|    clip_fraction        | 0.326       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.3       |\n",
            "|    explained_variance   | 0.411       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 45.6        |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | 0.00211     |\n",
            "|    reward               | -1.3194084  |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 83.7        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 100\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 7558561.70\n",
            "total_reward: 6558561.70\n",
            "total_cost: 164001.05\n",
            "total_trades: 67168\n",
            "Sharpe: 1.079\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 161          |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 1127         |\n",
            "|    total_timesteps      | 182272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.025494507  |\n",
            "|    clip_fraction        | 0.235        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -45.4        |\n",
            "|    explained_variance   | 0.368        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 46.1         |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.00609     |\n",
            "|    reward               | -0.036647968 |\n",
            "|    std                  | 1.16         |\n",
            "|    value_loss           | 133          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 90          |\n",
            "|    time_elapsed         | 1140        |\n",
            "|    total_timesteps      | 184320      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021608774 |\n",
            "|    clip_fraction        | 0.244       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.4       |\n",
            "|    explained_variance   | 0.129       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 126         |\n",
            "|    n_updates            | 890         |\n",
            "|    policy_gradient_loss | 0.00317     |\n",
            "|    reward               | -0.8471404  |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 156         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 161        |\n",
            "|    iterations           | 91         |\n",
            "|    time_elapsed         | 1153       |\n",
            "|    total_timesteps      | 186368     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03707566 |\n",
            "|    clip_fraction        | 0.347      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.4      |\n",
            "|    explained_variance   | 0.75       |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 6.42       |\n",
            "|    n_updates            | 900        |\n",
            "|    policy_gradient_loss | -0.00503   |\n",
            "|    reward               | 0.49877664 |\n",
            "|    std                  | 1.16       |\n",
            "|    value_loss           | 16.7       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 1166        |\n",
            "|    total_timesteps      | 188416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024886845 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.5       |\n",
            "|    explained_variance   | 0.412       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 106         |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | -0.00426    |\n",
            "|    reward               | -1.6618164  |\n",
            "|    std                  | 1.17        |\n",
            "|    value_loss           | 148         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 161        |\n",
            "|    iterations           | 93         |\n",
            "|    time_elapsed         | 1179       |\n",
            "|    total_timesteps      | 190464     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03060714 |\n",
            "|    clip_fraction        | 0.306      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.6      |\n",
            "|    explained_variance   | 0.298      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 68.4       |\n",
            "|    n_updates            | 920        |\n",
            "|    policy_gradient_loss | 0.00589    |\n",
            "|    reward               | -1.5559267 |\n",
            "|    std                  | 1.17       |\n",
            "|    value_loss           | 135        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 161         |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 1192        |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030435357 |\n",
            "|    clip_fraction        | 0.252       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.6       |\n",
            "|    explained_variance   | 0.545       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.1        |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | 0.00222     |\n",
            "|    reward               | -0.20128284 |\n",
            "|    std                  | 1.17        |\n",
            "|    value_loss           | 24.2        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 161          |\n",
            "|    iterations           | 95           |\n",
            "|    time_elapsed         | 1205         |\n",
            "|    total_timesteps      | 194560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.031035945  |\n",
            "|    clip_fraction        | 0.213        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -45.6        |\n",
            "|    explained_variance   | 0.267        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 123          |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | 0.00202      |\n",
            "|    reward               | -0.034586366 |\n",
            "|    std                  | 1.17         |\n",
            "|    value_loss           | 105          |\n",
            "------------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 161       |\n",
            "|    iterations           | 96        |\n",
            "|    time_elapsed         | 1218      |\n",
            "|    total_timesteps      | 196608    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0328287 |\n",
            "|    clip_fraction        | 0.262     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -45.7     |\n",
            "|    explained_variance   | 0.328     |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 59.4      |\n",
            "|    n_updates            | 950       |\n",
            "|    policy_gradient_loss | 0.000734  |\n",
            "|    reward               | 5.9145455 |\n",
            "|    std                  | 1.17      |\n",
            "|    value_loss           | 140       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 161        |\n",
            "|    iterations           | 97         |\n",
            "|    time_elapsed         | 1231       |\n",
            "|    total_timesteps      | 198656     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03959734 |\n",
            "|    clip_fraction        | 0.302      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.7      |\n",
            "|    explained_variance   | 0.359      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 19.6       |\n",
            "|    n_updates            | 960        |\n",
            "|    policy_gradient_loss | 0.00729    |\n",
            "|    reward               | 2.7647195  |\n",
            "|    std                  | 1.17       |\n",
            "|    value_loss           | 53.9       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 161        |\n",
            "|    iterations           | 98         |\n",
            "|    time_elapsed         | 1244       |\n",
            "|    total_timesteps      | 200704     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02602975 |\n",
            "|    clip_fraction        | 0.248      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.7      |\n",
            "|    explained_variance   | 0.161      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 47.7       |\n",
            "|    n_updates            | 970        |\n",
            "|    policy_gradient_loss | -0.00271   |\n",
            "|    reward               | 0.20029236 |\n",
            "|    std                  | 1.17       |\n",
            "|    value_loss           | 95.4       |\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo,\n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=200000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C6AidlWyvwzm"
      },
      "outputs": [],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JSAHhV4Xc-bh",
        "outputId": "d1ecbcd8-6e79-48f9-917f-66ef939acb50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "Logging to results/td3\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100,\n",
        "              \"buffer_size\": 1000000,\n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OSRxNYAxdKpU",
        "outputId": "bc7f8f1a-8b43-4cfd-ee7c-6e4bb4e9372a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day: 2892, episode: 110\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5157202.55\n",
            "total_reward: 4157202.55\n",
            "total_cost: 999.00\n",
            "total_trades: 40456\n",
            "Sharpe: 0.787\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 133       |\n",
            "|    time_elapsed    | 86        |\n",
            "|    total_timesteps | 11572     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 30.3      |\n",
            "|    critic_loss     | 23.2      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 11471     |\n",
            "|    reward          | 3.5435894 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 129       |\n",
            "|    time_elapsed    | 178       |\n",
            "|    total_timesteps | 23144     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 24        |\n",
            "|    critic_loss     | 89.1      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 23043     |\n",
            "|    reward          | 3.5435894 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 129       |\n",
            "|    time_elapsed    | 268       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 16        |\n",
            "|    critic_loss     | 17.9      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | 3.5435894 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 120\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5157202.55\n",
            "total_reward: 4157202.55\n",
            "total_cost: 999.00\n",
            "total_trades: 40456\n",
            "Sharpe: 0.787\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 129       |\n",
            "|    time_elapsed    | 358       |\n",
            "|    total_timesteps | 46288     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 16.1      |\n",
            "|    critic_loss     | 25.7      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 46187     |\n",
            "|    reward          | 3.5435894 |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3,\n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=50000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OkJV6V_mv2hw"
      },
      "outputs": [],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xwOhVjqRkCdM",
        "outputId": "782c71d3-fe9c-45f2-adbf-ad8abe15b8c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cuda device\n",
            "Logging to results/sac\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "K8RSdKCckJyH",
        "outputId": "1185dabf-5894-4979-a3ed-f0929fdcf30b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 89        |\n",
            "|    time_elapsed    | 129       |\n",
            "|    total_timesteps | 11572     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.44e+03  |\n",
            "|    critic_loss     | 180       |\n",
            "|    ent_coef        | 0.23      |\n",
            "|    ent_coef_loss   | -67.6     |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 11471     |\n",
            "|    reward          | 11.804635 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 130\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 7206732.73\n",
            "total_reward: 6206732.73\n",
            "total_cost: 126250.93\n",
            "total_trades: 66773\n",
            "Sharpe: 1.020\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 89       |\n",
            "|    time_elapsed    | 258      |\n",
            "|    total_timesteps | 23144    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 604      |\n",
            "|    critic_loss     | 541      |\n",
            "|    ent_coef        | 0.0754   |\n",
            "|    ent_coef_loss   | -96.4    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 23043    |\n",
            "|    reward          | 9.501004 |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 89        |\n",
            "|    time_elapsed    | 387       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 260       |\n",
            "|    critic_loss     | 63.9      |\n",
            "|    ent_coef        | 0.0249    |\n",
            "|    ent_coef_loss   | -83.6     |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | 10.599071 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 140\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 7236052.94\n",
            "total_reward: 6236052.94\n",
            "total_cost: 2655.82\n",
            "total_trades: 46865\n",
            "Sharpe: 1.035\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 89       |\n",
            "|    time_elapsed    | 517      |\n",
            "|    total_timesteps | 46288    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 132      |\n",
            "|    critic_loss     | 68.3     |\n",
            "|    ent_coef        | 0.0085   |\n",
            "|    ent_coef_loss   | -43.1    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 46187    |\n",
            "|    reward          | 8.642694 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 89       |\n",
            "|    time_elapsed    | 649      |\n",
            "|    total_timesteps | 57860    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 73.9     |\n",
            "|    critic_loss     | 17.1     |\n",
            "|    ent_coef        | 0.00419  |\n",
            "|    ent_coef_loss   | -4.69    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 57759    |\n",
            "|    reward          | 8.468915 |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-70ecaf5399b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trained_sac = agent.train_model(model=model_sac, \n\u001b[0m\u001b[1;32m      2\u001b[0m                              \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sac'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                              total_timesteps=70000) if if_using_sac else None\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/finrl/agents/stablebaselines3/models.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     ):  # this function is static method, so it can be called without creating an instance of the class\n\u001b[0;32m--> 117\u001b[0;31m         model = model.learn(\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/stable_baselines3/sac/sac.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     ) -> SelfSAC:\n\u001b[0;32m--> 308\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;31m# Special case when the user passes `gradient_steps=0`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/stable_baselines3/sac/sac.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;31m# Optimize the critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac,\n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=70000) if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SpZoQgPv7GO"
      },
      "outputs": [],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGm3dQZfRks"
      },
      "source": [
        "## Save the trained agent\n",
        "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
        "\n",
        "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
        "\n",
        "For users running on your local environment, the zip files should be at \"./trained_models\"."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRiOtrywfAo1",
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv",
        "Dr49PotrfG01"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}